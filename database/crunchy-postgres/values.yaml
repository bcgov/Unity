fullnameOverride: crunchy-postgres

# Set this to true for OpenShift deployments to avoid incompatible securityContext values
openshift: true

labels:
  app.kubernetes.io/part-of: crunchydb-postgres

crunchyImage: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
#crunchyImage: artifacts.developer.gov.bc.ca/bcgov-docker-local/crunchy-postgres-gis:ubi8-15.2-3.3-0 # use this image for POSTGIS
postgresVersion: 16
#postGISVersion: '3.3' # use this version of POSTGIS. both crunchyImage and this property needs to have valid values for POSTGIS to be enabled.
imagePullPolicy: IfNotPresent

# enable to bootstrap a standby cluster from backup. Then disable to promote this standby to primary
standby:
  enabled: false
  # If you want to recover from PVC, use repo1. If you want to recover from S3, use repo2
  repoName: repo1

instances:
  name: ha # high availability
  replicas: 2
  dataVolumeClaimSpec:
    storage: 512Mi
    storageClassName: netapp-block-standard
  requests:
    cpu: 10m
    memory: 256Mi
  replicaCertCopy:
    requests:
      cpu: 1m
      memory: 32Mi
    limits:
      cpu: 50m
      memory: 64Mi

# If we need to restore the cluster from a backup, we need to set the following values
# assuming restore from repo2 (s3), adjust as needed if your S3 repo is different
dataSource:
  enabled: false
  # should have the same name and contain the same keys as the pgbackrest secret
  secretName: s3-pgbackrest
  repo:
    name: repo2
    path: "/habackup"
    s3:
      bucket: "bucketName"
      endpoint: "https://sector.objectstore.gov.bc.ca"
      region: "not-used"
      uriStyle: "path"
    stanza: db

pgBackRest:
  image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
  # If retention-full-type set to 'count' then the oldest backups will expire when the number of backups reach the number defined in retention
  # If retention-full-type set to 'time' then the number defined in retention will take that many days worth of full backups before expiration
  retention: "2" # Ideally a number to keep backups for 2 working days
  retentionS3: "30" # Ideally a larger number such as backups for 30 days
  retentionFullType: count # Type of retention for full backups
  retentionFullTypeS3: time # Type of retention for full backups
  retentionArchive: "2" # Number of backups worth of continuous WAL to retain
  retentionArchiveType: full # Type of retention for WAL archives
  repos:
    schedules:
      full: 0 6 * * 0 # Full backup every Sunday at 10:00 PM PST
      incremental: 15 */8 * * * # Incremental every 8 hours
    volume:
      accessModes: "ReadWriteOnce"
      storage: 256Mi
      storageClassName: netapp-file-backup
  repoHost:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi
  sidecars:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi
  s3:
    enabled: true
    createS3Secret: true
    # the s3 secret name
    s3Secret: s3-pgbackrest
    # the path start with /, it will be created under bucket if it doesn't exist
    s3Path: "/habackup"
    # s3UriStyle is host or path
    s3UriStyle: path
    # bucket specifies the S3 bucket to use,
    bucket: "bucketName"
    # endpoint specifies the S3 endpoint to use.
    endpoint: "https://sector.objectstore.gov.bc.ca"
    # region specifies the S3 region to use. If your S3 storage system does not
    # use "region", fill this in with a random value.
    region: "not-used"
    # key is the S3 key. This is stored in a Secret.
    # Please DO NOT push this value to GitHub
    key: "s3keyValue"
    # keySecret is the S3 key secret. This is stored in a Secret.
    # Please DO NOT push this value to GitHub
    keySecret: "s3SecretValue"
    # set the default schedule to avoid conflicts
    fullSchedule: 30 5 * * 0 # Full backup every Monday at 9:30 PM PST
    incrementalSchedule: 45 */8 * * * # Incremental every 8 hours

patroni:
  postgresql:
    pg_hba:
      - "local all postgres trust" # trust local system socket connections user postgres
      - "host all all 127.0.0.1/32 trust" # trust IPv4 local connections includes port forwarding
      - "host all all ::1/128 trust" # trust IPv6 local connections includes port forwarding
      - "host all all 10.0.0.0/8 md5" # Allow any users to connect to any database from 10.x.x.x private subnet range if password is correctly supplied
    parameters:
      shared_buffers: 256MB # default is 128MB; a good tuned default for shared_buffers is 25% of the memory allocated to the pod
      wal_buffers: "-1" # this can be set to -1 to automatically set as 1/32 of shared_buffers or 64kB, whichever is larger
      min_wal_size: 64MB # Sets the minimum size to shrink the WAL files to
      max_wal_size: 256MB # default is 1GB make sure the mounted volume is large enough for the logging
      max_slot_wal_keep_size: 256MB # default is -1, allowing unlimited wal growth when replicas fall behind
      temp_file_limit: 512MB  # Prevent temp files from filling PVC
      checkpoint_timeout: 15min  # Reduce checkpoint frequency
      checkpoint_completion_target: 0.9  # Smooth checkpointing

proxy:
  pgBouncer:
    image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
    replicas: 2
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi

# Postgres Cluster resource values:
pgmonitor:
  enabled: false
  exporter:
    image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi

# Data restore cronjob configuration - reuses dataSource and pgBackRest.s3 patterns
dataRestore:
  enabled: false
  createS3Secret: true
  schedule: "0 2 * * *" # Run every day at 2 AM
  image: "artifacts.developer.gov.bc.ca/bcgov-docker-local/crunchy-pgbackrest:ubi8-2.53.1-0"
  secretName: s3-pgbackrest
  repo:
    name: repo2
    path: "/habackup"
    bucket: "bucketName"
    endpoint: "https://sector.objectstore.gov.bc.ca"
    region: "not-used"
    uriStyle: "path"
  stanza: db
  # S3 credentials for data restore (only used if createS3Secret: true)
  s3:
    # key is the S3 key. This is stored in a Secret.
    # Please DO NOT push this value to GitHub
    key: "s3keyValue"
    # keySecret is the S3 key secret. This is stored in a Secret.
    # Please DO NOT push this value to GitHub
    keySecret: "s3SecretValue"
  # Target database configuration
  target:
    # The PostgreSQL cluster name to restore into (defaults to current cluster if empty)
    clusterName: ""
    # Database name to restore
    database: "postgres"
  # Resource limits for the cronjob
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  # Job settings
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  restartPolicy: OnFailure
  # Additional pgbackrest arguments
  additionalArgs: []
    # - "--log-level-console=debug"
    # - "--process-max=2"
